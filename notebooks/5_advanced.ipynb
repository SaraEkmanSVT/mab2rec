{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb407df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "os.chdir('..')\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mab2rec import BanditRecommender, LearningPolicy, NeighborhoodPolicy\n",
    "\n",
    "tmp_dir = \"tmp_dir\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc30e0",
   "metadata": {},
   "source": [
    "# Advanced\n",
    "\n",
    "* The goal of this notebook is to present an overview of advanced functionality. \n",
    "* **Mab2Rec Recommender:** How to interact with a Mab2Rec recommender object and brief overview of the bandit literature.\n",
    "* **Persistency:** How to persist trained recommender algorithms.\n",
    "* **Item Control:** Specifying items to use at both the *recommender-level* and at the *user-level*.\n",
    "    * **Recommender level**: The list of items to train or score can be specified. If this list includes items not in the training data (or trained recommender), these items can be **warm started** given input `item_features`.\n",
    "    * **User level**: For scoring, one can also specify a list of **eligible items** for each user that the recommender will respect.\n",
    "* **Memory Efficiency:** The user features input can consume significant memory. The `user_features_dtypes` argument can be used to cast input features to data types that consume less memory. This is especially useful when many binary indicator features are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f9a08",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Input Data](#Input-Data)\n",
    "2. [Mab2Rec Recommender](#Mab2Rec-Recommender)\n",
    "    1. [Input](#Input)\n",
    "    2. [Arms](#Arms)\n",
    "    3. [Fit](#Train)\n",
    "    4. [Recommend](#Recommend)\n",
    "    5. [Warm Start](#Warm-Start)\n",
    "3. [Persistency](#Persistency)\n",
    "4. [Item Control](#Item-Control)\n",
    "5. [Memory Efficiency](#Memory-Efficiency)\n",
    "6. [Appendix: Intro to Bandits](#Appendix:-Intro-to-Bandits)\n",
    "    1. [Context-Free Bandits](#Context-Free-Bandits)\n",
    "    2. [Contextual Parametric Bandits](#Contextual-Parametric-Bandits)\n",
    "    3. [Contextual Non-parametric Bandits](#Contextual-Non-Parametric-Bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57904669",
   "metadata": {},
   "source": [
    "# Input Data\n",
    "\n",
    "* Input data is as described in [Data Overview](https://github.com/fidelity/mab2rec/blob/main/notebooks/1_data_overview.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75606e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic input data\n",
    "train_data = \"data/data_train.csv\"\n",
    "test_data = \"data/data_test.csv\"\n",
    "user_features = \"data/features_user.csv\"\n",
    "item_features = \"data/features_item.csv\"\n",
    "\n",
    "# Extended input data\n",
    "eligibility_data = \"data/extended/data_eligibility.csv\"\n",
    "user_features_dtypes = \"data/extended/features_user_dtypes.json\"\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv(train_data)\n",
    "test_df = pd.read_csv(test_data)\n",
    "user_features_df = pd.read_csv(user_features)\n",
    "item_features_df = pd.read_csv(item_features)\n",
    "eligibility_df = pd.read_csv(eligibility_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5032247",
   "metadata": {},
   "source": [
    "# Mab2Rec Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5f64c",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd6c8",
   "metadata": {},
   "source": [
    "* The mapping between bandit input parameters and recommender input parameters are as follows: \n",
    "    * **Decisions** correspond to `item_id` column in the `data_train.csv`. \n",
    "    * **Rewards** correspond to `response` column in the `data_train.csv`. \n",
    "    * **Contexts** correspond to user features in the `features_user.csv` for each `user_id` in the `data_train.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06a967",
   "metadata": {},
   "source": [
    "## Arms\n",
    "* When a recommender is created it has no knowledge of the items, i.e., `arms`, that can be recommended.\n",
    "* During the `fit` operation, the `arms` of the recommender are initialized with unique items found in `decisions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239964f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item1', 'Item2', 'Item3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "contexts = [[0, 0, 0], [1, 0, 1], [0, 1, 1], [0, 0, 0], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Training: Fit operations initializes the arms\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "rec.mab.arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0cbd",
   "metadata": {},
   "source": [
    "* Alternatively, the arms of a recommender can be **set** directly as follows.\n",
    "* The `set_arms` function simply performs a series of `add_arm` and `remove_arm` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ca6eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item2', 'Item3', 'Item4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Set arms explicitly\n",
    "rec.set_arms(['Item2', 'Item3', 'Item4'])\n",
    "\n",
    "rec.mab.arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb6480",
   "metadata": {},
   "source": [
    "* Arms can also be **added** or **removed** from the recommender as required.\n",
    "* Added arms will not have any training data or parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5154003b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item2', 'Item3', 'Item4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Training: Fit operations initializes the arms\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "# Add arm\n",
    "rec.add_arm(\"Item4\")\n",
    "\n",
    "# Remove arm\n",
    "rec.remove_arm(\"Item1\")\n",
    "\n",
    "rec.mab.arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf21b8",
   "metadata": {},
   "source": [
    "## Fit \n",
    "\n",
    "* All bandit policies require historical `decisions` and corresponding `rewards` to be trained.\n",
    "* Contextual learning policies and neighborhood policies require additional `contexts` data for training.\n",
    "* The `fit` function is used for the initial training. \n",
    "* Following model updates are performed through `partial_fit` for continuous learning.\n",
    "* The time required to fit a recommender depends on the bandit policy, the number of samples and the features in the context, if contextual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67103ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "contexts = [[0, 0, 0], [1, 0, 1], [0, 1, 1], [0, 0, 0], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Training: Fit operation for the initial training\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "# New feedback data becomes available\n",
    "decisions_new = ['Item4', 'Item1', 'Item3']\n",
    "rewards_new = [1, 1, 0]\n",
    "contexts_new = [[0, 1, 1], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# Partial fit for continuous learning\n",
    "rec.partial_fit(decisions_new, rewards_new, contexts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce267ae",
   "metadata": {},
   "source": [
    "## Recommend\n",
    "\n",
    "* Recommend operation returns the top-*k* arms, or a list of arms if multiple contexts are given, based on the expected reward.\n",
    "* The definition of top-*k* depends on the specified learning policy.\n",
    "* Optionally, scores associated with each recommend arm can be returned via `return_scores` parameter.\n",
    "* Again, the definition of the score will depend on the specified learning policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "449ec18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Item2', 'Item1'], ['Item2', 'Item1'], ['Item1', 'Item2']],\n",
       " [[0.6224593312018546, 0.5825702064623147],\n",
       "  [0.679178699175393, 0.6607563687658172],\n",
       "  [0.6607563687658172, 0.6224593312018546]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "contexts = [[0, 0, 0], [1, 0, 1], [0, 1, 1], [0, 0, 0], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Training: Fit operation for the initial training\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "# Generate top-k recommendation for each context\n",
    "rec.recommend([[1, 1, 0], [1, 1, 1], [1, 0, 1]], return_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f56030",
   "metadata": {},
   "source": [
    "* Specific arms can be excluded for each individual context recommended via the `excluded_arms` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54c2ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Item2', 'Item3'], ['Item2', 'Item1'], ['Item1', 'Item2']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excluded arms for each individual user/context\n",
    "excluded_arms = [['Item1'], [], ['Item3']]\n",
    "\n",
    "# Generate top-k recommendation for each context\n",
    "rec.recommend([[1, 1, 0], [1, 1, 1], [1, 0, 1]], excluded_arms=excluded_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674040de",
   "metadata": {},
   "source": [
    "## Warm Start\n",
    "\n",
    "* The [cold-start problem](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)) refers to situations where some items to be recommended have no historic data.\n",
    "* The bandit recommender provides a simple warm start procedure that can be used to warm start cold arms using trained arms. \n",
    "* To run warm start, features of each of the recommender arms are required. \n",
    "* See [Feature Engineering Notebook](https://github.com/fidelity/mab2rec/blob/main/notebooks/2_feature_engineering.ipynb) for more details on creating item features.\n",
    "* A distance parameter, `distance_quantile`, specifies how \"close\" a cold arm has to be to a warm arm is also required. \n",
    "* If the distance is set to 1.0, all cold items will be warm started and none will be warm started if it is set to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d010501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Item2', 'Item4'], ['Item2', 'Item4'], ['Item4', 'Item1']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "contexts = [[0, 0, 0], [1, 0, 1], [0, 1, 1], [0, 0, 0], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# Bandit: LinGreedy learning policy with epsilon set to 0.1 and top-2\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "\n",
    "# Training: Fit operation for the initial training\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "# Add new arm with no data\n",
    "rec.add_arm('Item4')\n",
    "\n",
    "# Warm start\n",
    "arm_to_features = {'Item1': [1, 1], 'Item2': [1, 0], 'Item3': [0.5, 0.5], 'Item4': [1, 1]}\n",
    "rec.warm_start(arm_to_features, distance_quantile=0.75)\n",
    "\n",
    "# Generate top-k recommendation for each context\n",
    "rec.recommend([[1, 1, 0], [1, 1, 1], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593ee77",
   "metadata": {},
   "source": [
    "# Persistency\n",
    "\n",
    "## Save to Pickle\n",
    "\n",
    "* Pickling the BanditRecommender object is an easy way to store the whole model within a single file.\n",
    "* This can be achieved by giving an output pickle path to the `train` pipeline. \n",
    "* Alternatively, we can run `save_pickle` on the BanditRecommender object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8856396b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mab2rec import BanditRecommender, LearningPolicy\n",
    "from mab2rec.pipeline import train, score\n",
    "from mab2rec.utils import save_pickle, load_pickle\n",
    "\n",
    "# Create recommender with LinGreedy regression policy\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=4)\n",
    "\n",
    "# Pickle path to save the artifact\n",
    "pickle_path = os.path.join(tmp_dir, 'rec.pkl')\n",
    "\n",
    "# Train and save BanditRecommender\n",
    "train(rec, data='data/data_train.csv', \n",
    "      user_features='data/features_user.csv',\n",
    "      save_file=pickle_path)\n",
    "\n",
    "# Alternatively, the returned BanditRecommender object could also be saved directly\n",
    "save_pickle(rec, pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44728c3f",
   "metadata": {},
   "source": [
    "## Load from Pickle\n",
    "\n",
    "* Loading from the saved pickle file is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9481dbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>483</td>\n",
       "      <td>0.672446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259</td>\n",
       "      <td>474</td>\n",
       "      <td>0.656253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259</td>\n",
       "      <td>12</td>\n",
       "      <td>0.651795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259</td>\n",
       "      <td>64</td>\n",
       "      <td>0.650715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851</td>\n",
       "      <td>313</td>\n",
       "      <td>0.701218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id     score\n",
       "0      259      483  0.672446\n",
       "1      259      474  0.656253\n",
       "2      259       12  0.651795\n",
       "3      259       64  0.650715\n",
       "4      851      313  0.701218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load BanditRecommender from pickle file\n",
    "rec = load_pickle(pickle_path)\n",
    "\n",
    "# Recommendations from loaded model\n",
    "df = score(rec, data='data/data_test.csv', \n",
    "           user_features='data/features_user.csv')\n",
    "\n",
    "# Results\n",
    "display(df.head())\n",
    "\n",
    "# Clean-up\n",
    "os.remove(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8091ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870e159",
   "metadata": {},
   "source": [
    "# Item Control\n",
    "\n",
    "## Train Items\n",
    "\n",
    "* The list of items to be trained can be specified using the `item_list` argument in the **train** function.\n",
    "* This allows one to only train a subset of the items that occur in the training data.\n",
    "* Conversely, it can also be used to specify items that do not occur in the training data, which could be warm-started using appropriate `item_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d8c84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items (train data):  201\n",
      "Num items (recommender):  10\n"
     ]
    }
   ],
   "source": [
    "# Scenario: Train a subset of items in train data\n",
    "\n",
    "# List of 10 items to train\n",
    "item_list = train_df['item_id'].unique()[:10].tolist()\n",
    "\n",
    "# Train\n",
    "rec = BanditRecommender(LearningPolicy.Random())\n",
    "train(rec, data=train_df, item_list=item_list)\n",
    "\n",
    "print(\"Num items (train data): \", train_df['item_id'].nunique())\n",
    "print(\"Num items (recommender): \", len(rec.mab.arms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deded912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items (train data):  161\n",
      "Num items (recommender):  201\n"
     ]
    }
   ],
   "source": [
    "# Scenario: Train a superset of items in train data and then warm-start\n",
    "\n",
    "# Sample training data\n",
    "train_sample_df = train_df.sample(frac=0.01, random_state=12)\n",
    "\n",
    "# Set item list to be all items in the full training data\n",
    "item_list = train_df['item_id'].unique().tolist()\n",
    "\n",
    "# Train\n",
    "rec = BanditRecommender(LearningPolicy.Random())\n",
    "train(rec, data=train_sample_df, item_list=item_list, item_features=item_features_df,\n",
    "      warm_start=True, warm_start_distance=0.75)\n",
    "\n",
    "print(\"Num items (train data): \", train_sample_df['item_id'].nunique())\n",
    "print(\"Num items (recommender): \", len(rec.mab.arms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28667dc2",
   "metadata": {},
   "source": [
    "## Score Items\n",
    "\n",
    "* Similarly, the list of items to recommend can also be specified using the `item_list` argument in the **score** function.\n",
    "* Items that are not in the `item_list` will be removed from the recommender and items in the `item_list` not in the recommender will be added. \n",
    "* Same as above, if untrained (cold) items are added to the recommender, warm start can be used with the corresponding `item_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d06df99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items (recommender):  201\n",
      "Num items (scored recommendations):  10\n"
     ]
    }
   ],
   "source": [
    "# Scenario: Score using a subset of items in trained recommender\n",
    "\n",
    "# Train recommender on ALL data (i.e., no item list)\n",
    "rec = BanditRecommender(LearningPolicy.Random())\n",
    "train(rec, data=train_df)\n",
    "print(\"Num items (recommender): \", len(rec.mab.arms))\n",
    "\n",
    "# Score only 10 items\n",
    "item_list = train_df['item_id'].unique()[:10].tolist()\n",
    "df = score(rec, data=test_df, item_list=item_list)\n",
    "print(\"Num items (scored recommendations): \", df['item_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cca4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num items (recommender):  161\n",
      "Num items (scored recommendations):  201\n"
     ]
    }
   ],
   "source": [
    "# Scenario: Score superset of items in trained recommender and then warm-start\n",
    "\n",
    "# Sample training data\n",
    "train_sample_df = train_df.sample(frac=0.01, random_state=12)\n",
    "\n",
    "# Train using sample data\n",
    "rec = BanditRecommender(LearningPolicy.Random())\n",
    "train(rec, data=train_sample_df)\n",
    "print(\"Num items (recommender): \", len(rec.mab.arms))\n",
    "\n",
    "# Score ALL items in data after warm start\n",
    "item_list = train_df['item_id'].unique().tolist()\n",
    "df = score(rec, data=test_df, item_list=item_list, item_features=item_features_df,\n",
    "           warm_start=True, warm_start_distance=0.75)\n",
    "print(\"Num items (scored recommendations): \", df['item_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b673b3",
   "metadata": {},
   "source": [
    "## Item Eligibility per User\n",
    "\n",
    "* It is also possible to account for more fine-grained **item eligibility**.\n",
    "* This becomes important for ensuring that the user gets a recommendation that they haven't seen, or when certain items have very strict requirements on who is allowed to see them.\n",
    "* To use this functionality, a list of eligible items is required for each user, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403fe0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>[432, 181, 302, 147, 186, 96, 550, 248, 498, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>851</td>\n",
       "      <td>[474, 4, 591, 660, 132, 268, 218, 265, 133, 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>712</td>\n",
       "      <td>[479, 323, 333, 69, 322, 471, 269, 187, 433, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>[248, 194, 238, 118, 603, 427, 326, 185, 283, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>640</td>\n",
       "      <td>[591, 402, 153, 582, 603, 432, 89, 298, 451, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            item_id\n",
       "0      259  [432, 181, 302, 147, 186, 96, 550, 248, 498, 2...\n",
       "1      851  [474, 4, 591, 660, 132, 268, 218, 265, 133, 49...\n",
       "2      712  [479, 323, 333, 69, 322, 471, 269, 187, 433, 2...\n",
       "3      119  [248, 194, 238, 118, 603, 427, 326, 185, 283, ...\n",
       "4      640  [591, 402, 153, 582, 603, 432, 89, 298, 451, 3..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligibility_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41bc3f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>134</td>\n",
       "      <td>0.729780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259</td>\n",
       "      <td>70</td>\n",
       "      <td>0.724034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259</td>\n",
       "      <td>188</td>\n",
       "      <td>0.719724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259</td>\n",
       "      <td>132</td>\n",
       "      <td>0.719464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>523</td>\n",
       "      <td>0.718629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id     score\n",
       "0      259      134  0.729780\n",
       "1      259       70  0.724034\n",
       "2      259      188  0.719724\n",
       "3      259      132  0.719464\n",
       "4      259      523  0.718629"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "rec = BanditRecommender(LearningPolicy.Random())\n",
    "train(rec, data=train_df)\n",
    "\n",
    "# Make recommendations that satisfy eligibility criteria\n",
    "df = score(rec, data=test_df, item_eligibility=eligibility_df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dd2ed",
   "metadata": {},
   "source": [
    "# Memory Efficiency\n",
    "\n",
    "- When working with large datasets, the memory-intensive default data types of DataFrames can cause out-of-memory errors, or lead to suboptimal performance.\n",
    "- Mab2Rec allows data types to be specified for user features.\n",
    "- The [Selective](https://github.com/fidelity/selective) library offers a [memory reduction utility](https://github.com/fidelity/selective/blob/master/feature/utils.py#L189) that can generate a DataFrame with lower memory footprint.\n",
    "- Mab2Rec can directly utilize the data types of the DataFrame with reduced memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c6e0c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'uint16',\n",
       " 'u0': 'uint8',\n",
       " 'u1': 'uint8',\n",
       " 'u2': 'uint8',\n",
       " 'u3': 'uint8',\n",
       " 'u4': 'uint8',\n",
       " 'u5': 'uint8',\n",
       " 'u6': 'uint8',\n",
       " 'u7': 'uint8',\n",
       " 'u8': 'uint8',\n",
       " 'u9': 'uint8',\n",
       " 'u10': 'uint8',\n",
       " 'u11': 'uint8',\n",
       " 'u12': 'uint8',\n",
       " 'u13': 'uint8',\n",
       " 'u14': 'uint8',\n",
       " 'u15': 'uint8',\n",
       " 'u16': 'uint8',\n",
       " 'u17': 'uint8',\n",
       " 'u18': 'uint8',\n",
       " 'u19': 'uint8',\n",
       " 'u20': 'uint8',\n",
       " 'u21': 'uint8',\n",
       " 'u22': 'uint8',\n",
       " 'u23': 'uint8',\n",
       " 'u24': 'uint8',\n",
       " 'u25': 'uint8',\n",
       " 'u26': 'uint8',\n",
       " 'u27': 'uint8',\n",
       " 'u28': 'uint8',\n",
       " 'u29': 'uint8',\n",
       " 'u30': 'uint8',\n",
       " 'u31': 'uint8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(user_features_dtypes) as fp:\n",
    "    display(json.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24e1fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommender with LinUCB regression policy\n",
    "rec = BanditRecommender(LearningPolicy.LinUCB(alpha=1.25))\n",
    "\n",
    "# Train using specified data types for user features\n",
    "train(rec, data='data/data_train.csv', \n",
    "      user_features='data/features_user.csv',\n",
    "      user_features_dtypes=user_features_dtypes)\n",
    "\n",
    "# Train using specified data types for user features\n",
    "df = score(rec, data='data/data_test.csv', \n",
    "            user_features='data/features_user.csv',\n",
    "            user_features_dtypes=user_features_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c65480",
   "metadata": {},
   "source": [
    "# Appendix: Intro to Bandits\n",
    "\n",
    "## Context-Free Bandits\n",
    "\n",
    "* Context-free bandits can be used as simple baseline recommenders.\n",
    "* They are fit using data of items recommended in the past, i.e, `decisions`, and corresponding responses observed for each item, i.e., `rewards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e4e3552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item2', 'Item3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Epsilon Greedy learning policy with random exploration set to 25%\n",
    "rec = BanditRecommender(LearningPolicy.EpsilonGreedy(epsilon=0.25), top_k=2)\n",
    "rec.fit(decisions, rewards)\n",
    "\n",
    "# Generate top-k recommendation\n",
    "rec.recommend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9fade",
   "metadata": {},
   "source": [
    "## Contextual Parametric Bandits\n",
    "\n",
    "* Parametric bandits assume rewards to be random and distributed independently according to a probability distribution that is specific to each arm.\n",
    "* The expected reward for each arm is estimated using a **parametric** model.\n",
    "* The `LinGreedy` learning policy computes the expected reward of each arm by finding a linear combination of the previous rewards of the arm.\n",
    "* The policy selects the top-*k* arms based on the predicted regression values with probability 1 - $\\epsilon$ and random arms with probability $\\epsilon$.\n",
    "* Parametric bandits are typically very efficient.\n",
    "* They are fit using data of items recommended in the past (i.e, `decisions`), the corresponding response observed for each recommended item (i.e., `rewards`) and user features (i.e., `contexts`) associated with each of the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bed79044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Item2', 'Item1'], ['Item2', 'Item1'], ['Item2', 'Item3']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "decisions = ['Item1', 'Item1', 'Item3', 'Item1', 'Item2', 'Item3']\n",
    "rewards = [0, 1, 1, 0, 1, 0]\n",
    "contexts = [[0, 0, 0], [1, 0, 1], [0, 1, 1], [0, 0, 0], [1, 1, 1], [0, 1, 0]]\n",
    "\n",
    "# LinGreedy learning policy with epsilon set to 0.1\n",
    "rec = BanditRecommender(LearningPolicy.LinGreedy(epsilon=0.1), top_k=2)\n",
    "rec.fit(decisions, rewards, contexts)\n",
    "\n",
    "# Generate top-k recommendation for each context\n",
    "rec.recommend([[1, 1, 0], [1, 1, 1], [0, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
